<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Brayan Impata</title>
	<link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,900" rel="stylesheet">
    <link rel="stylesheet" href="libs/font-awesome/css/font-awesome.min.css">
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/styles.css" rel="stylesheet">
</head>

<body>
    <div id="mobile-menu-open" class="shadow-large">
        <i class="fa fa-bars" aria-hidden="true"></i>
    </div>
    <!-- End #mobile-menu-toggle -->
    <header>
        <div id="mobile-menu-close">
            <span>Close</span> <i class="fa fa-times" aria-hidden="true"></i>
        </div>
        <ul id="menu" class="shadow">
            <li>
                <a href="#about">About</a>
            </li>
            <li>
                <a href="#projects">Projects</a>
            </li>
            <li>
                <a href="#skills">Skills</a>
            </li>
            <li>
                <a href="#publications">Publications</a>
            </li>
        </ul>
    </header>
    <!-- End header -->

    <div id="lead">
        <div id="lead-content">
            <img src="images/profile.png" height="290px" width="290px" style="margin-bottom: 15px">
            <h1 style="margin-bottom: 15px">Brayan Impata</h1>
            <h2 style="color:white">Researcher & Engineer</h2>
            <center>
                <div>
                    <a href="https://twitter.com/_yayan24" target="_blank"><img src="images/twitter.svg" height="65px" width="65px" style="float: center; vertical-align:middle; padding: 6px"></a>
                    <a href="https://github.com/yayaneath" target="_blank"><img src="images/github.svg" height="65px" width="65px" style="float: center; vertical-align:middle; padding: 6px"></a>
                    <a href="https://www.linkedin.com/in/brayan-stiven-zapata-impata" target="_blank"><img src="images/linkedin.svg" height="65px" width="65px" style="float: center; vertical-align:middle; padding: 6px"></a>
                </div>
            </center>
        </div>
        <!-- End #lead-content -->

        <div id="lead-overlay"></div>

        <div id="lead-down">
            <span>
                <i class="fa fa-chevron-down" aria-hidden="true"></i>
            </span>
        </div>
        <!-- End #lead-down -->
    </div>
    <!-- End #lead -->

    <div id="about">
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <h2 class="heading">About Me</h2>
                </div>
                <div class="col-md-8">
                    <p>
                        I am <a href="https://scholar.google.com/citations?user=JCLdiOcAAAAJ" target="_blank">Brayan S. Zapata-Impata</a>, <b>PhD in Robotics and Machine Learning</b> by the University of Alicante (Spain). As a PhD with a background in engineering, I am mainly interested in solving real-world problems with innovative solutions. During my PhD thesis, I worked on providing robots with manipulation skills which are natural for us humans, like finding ways to grasp objects on the fly using <b>visual perception</b> and checking the stability of a grasp with <b>tactile perception</b>. Delivering robots with such autonomous abilities would have a great impact on a wide set of scenarios: from re-stocking robots to space explorers as well as robots helping us at home.
                        <br />
                        <br />
                        I consider myself a doer who knows that team work is paramount for achieving big goals. I am passionate about learning and I am continuously looking for challenges. I can organise myself effectively to deliver results, even out of my comfort area. During my free time, <b>I love reading books, listening and playing music, and walking my dog</b>.
                    </p>
                    <br />
                    <a class="btn-rounded-white" href="https://github.com/yayaneath/yayaneath.github.io/raw/master/docs/cv-brayan-impata.pdf" target="_blank">Download Resume</a>
                </div>
            </div>
        </div>
    </div>
    <!-- End #about -->

    <div id="projects" class="background-alt">
        <h2 class="heading">Projects</h2>
        <div class="container">
            <div class="row">
                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/geograsp.jpg" width="300px" height="300px" />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h3>Grasping Points Calculus: GeoGrasp</h3>
                        <p>
                            <b>GeoGrasp</b> is one of the first results of my PhD. It is a method for computing grasping points on partial views of objects using 3D point clouds. It performs a fast geometrical analysis which generalises to a wide range of objects. Consequently, no learning phase is required and it can pe rapidly applied on various scenarios. Right now, it is being used as the base grasping system for the European project <a href="http://commandia.unizar.es/" target="_blank">COMMANDIA</a>. I currently maintain it as an open-source ROS package written in C++.
                        </p>

                        <a href="https://github.com/yayaneath/GeoGrasp" target="_blank">View Project Page</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->

                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/grasp-stability.jpg" width="300px" height="300px" />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h3>Stability Prediction based on Tactile Images and Graphs</h3>
                        <p>
                            Computing visually promising grasps is not enough for developing an autonomous robot. In this project, I have worked on the task of predicting the outcome of a grasp before lifting using the BioTac SP tactile sensors and the Shadow Robot hand. To this end, I have explored two ways for representing the tactile responses: building tactile images or creating graphs that encode the actual spatial distribution of the sensing points.
                        </p>
                        <a href="https://github.com/yayaneath/biotac-sp-images" target="_blank">View Project Page (Images)</a><span style="display:inline-block; width: 15px;"></span><a href="https://github.com/3dperceptionlab/tactile-gcn" target="_blank">View Project Page (Graphs)</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->

                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/task-dos.jpg" width="300px" height="300px" />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h3>Direction of Slip Detection with Temporal Tactile Data</h3>
                        <p>
                            I have explored further the possibilities of tactile perception for grasp assessment. It is important for the autonomy of a robot to recognise whether a grasped object is slipping and also detect in which direction. This would help the system trigger appropriate re-grasping strategies for keeping a stable grip. This problem was approached using temporal sequences of tactile responses for learning to detect up to 7 different slippage patterns.
                        </p>

                        <a href="https://github.com/yayaneath/BioTacSP-DoS" target="_blank">View Project Page</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->

                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/autotrans.jpg" width="300px" height="300px" />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h3>AutOTrans: Autonomous Open world Transportation System</h3>
                        <p>
                            In many cities, trash bags often accumulate through the week, waiting to be picked up. Farm workers must often pick up and carry heavy tools daily. Construction workers spend a lot of time transporting materials through the construction site. These are all labor intensive tasks that could benefit from mobile robotic manipulation. For this project, I focused on the problem of picking and dropping novel objects in an open world environment. This work was carried during a research internship at the <a href="http://www.ccs.neu.edu/home/rplatt/" target="_blank">Helping Hands Lab head by Dr. Robert Platt</a>.
                        </p>

                        <a href="https://github.com/yayaneath/autotrans" target="_blank">View Project Page</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->

                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/touch-sight.jpg" width="300px" height="300px" />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h3>Feeling Touch by Sight</h3>
                        <p>
                            Latest trends in robotic grasping combine vision and touch for improving the performance of systems at tasks like stability prediction. However, tactile data are only available during the grasp, limiting the set of scenarios in which multi-modal solutions can be applied. Could we obtain it prior to grasping? We humans can estimate the feeling of objects without even making contact with them! In this on-going project I work on novel ways for generating robotic tactile data from visual perception.
                        </p>

                        <a href="https://github.com/yayaneath/vision2tactile" target="_blank">View Project Page</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->

                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/rl.jpg" width="300px" height="300px" />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h3>Reinforcement Learning</h3>
                        <p>
                            Humans are expert manipulators. Our dexterity is something we train since we are born: babies continuously use their hands to reach objects and learn to grasp them. Inspired in this fact, researchers are exploring the application of Reinforcement Learning (RL) for teaching robots how to grasp objects. In my path to learn about this field, I am implementing by myself RL techniques and testing them on common robotics benchmarks like a balancing robot. My goal is to develop my understanding of the field and apply these techniques for learning dexterous manipulation skills like re-grasping strategies based on tactile observations.
                        </p>
                        <a href="https://github.com/yayaneath/into-rl" target="_blank">View Project Page</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->
            </div>
        </div>
    </div>
    <!-- End #projects -->

    <div id="skills">
        <h2 class="heading">Skills</h2>
        <ul>
            <li>C</li>
            <li>C++</li>
            <li>Python</li>
            <li>Matlab</li>
            <li>LaTeX</li>
            <li>Linux</li>
            <li>Keras</li>
            <li>PyTorch</li>
            <li>MXNet</li>
            <li>ROS</li>
            <li>PCL</li>
            <li>OpenCV</li>
            <li>AWS</li>
            <li>Git</li>
            <li>Machine Learning</li>
            <li>Deep Learning</li>
            <li>Computer Vision</li>
            <li>Robotics</li>
        </ul>
    </div>
    <!-- End #skills -->

    <div id="publications" class="optional-section background-alt">
        <h2 class="heading">Publications</h2>

        <div class="optional-section-block">
            <h3>Journals</h3>
            <ul>
                <li><b>Zapata-Impata, B. S.</b>, Gil, P., Mezouar, Y. & Torres, F. (2020). <i>Generation of Tactile Data from 3D Vision and Target Robotic Grasps.</i> IEEE Transactions on Haptics. <a href="https://doi.org/10.1109/TOH.2020.3011899" target="_blank">https://doi.org/10.1109/TOH.2020.3011899</a></li>
                <br />

                <li>Velasco, E., <b>Zapata-Impata, B. S.</b>, Gil, P., & Torres, F. (2020). <i>Object classification using bimodal perception data extracted from single-touch robotic grasps.</i> Revista Iberoamericana de Automatica e Informatica Industrial, 17(1). <a href="https://doi.org/10.4995/riai.2019.10923" target="_blank">https://doi.org/10.4995/riai.2019.10923</a></li>
                <br />
                
                <li><b>Zapata-impata, B. S.</b>, Gil, P., & Torres, F. (2019). <i>Tactile-Driven Grasp Stability and Slip Prediction.</i> Robotics, 8(4), 85. <a href="https://doi.org/10.3390/robotics8040085" target="_blank">https://doi.org/10.3390/robotics8040085</a></li>
                <br />
                
                <li><b>Zapata-Impata, B. S.</b>, Gil, P., Pomares, J., & Torres, F. (2019). <i>Fast geometry-based computation of grasping points on three-dimensional point clouds.</i> International Journal of Advanced Robotic Systems, 16(1). <a href="https://doi.org/10.1177/1729881419831846" target="_blank>">https://doi.org/10.1177/1729881419831846</a></li>
                <br />

                <li><b>Zapata-Impata, B. S.</b>, Gil, P., & Torres, F. (2019). Learning Spatio Temporal Tactile Features with a ConvLSTM for the Direction Of Slip Detection. Sensors, 19(3), 1–16. <a href="https://doi.org/10.3390/s19030523" target="_blank">https://doi.org/10.3390/s19030523</a></li>
                <br />

                <li>Úbeda, A., <b>Zapata-Impata, B. S.</b>, Puente, S. T., Gil, P., Candelas, F., & Torres, F. (2018). A Vision-Driven Collaborative Robotic Grasping System Tele-Operated by Surface Electromyography. Sensors, 18(7), 2366. <a href="https://doi.org/10.3390/s18072366" target="_blank">https://doi.org/10.3390/s18072366</a></li>
            </ul>
        </div>
        <!-- End .optional-section-block -->

        <div class="optional-section-block">
            <h3>Conferences</h3>
            <ul>
                <li><b>Zapata-Impata, B. S.</b>, & Gil, P. (2020). <i>Prediction of Tactile Perception from Vision on Deformable Objects.</i> In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2020). WORKSHOP ROMADO (RObotic MAnipulation of Deformable Objects). <a href="https://rua.ua.es/dspace/bitstream/10045/109511/1/final-impata-gil-romado-iros2020.pdf" target="_blank">[Paper]</a></li>
                <br />

                <li>Garcia-Garcia, A., <b>Zapata-Impata, B. S.</b>, Orts-Escolano, S., Gil, P., & Garcia-Rodriguez, J. (2019). <i>TactileGCN: A Graph Convolutional Network for Predicting Grasp Stability with Tactile Sensors.</i> In International Joint Conference on Neural Networks (IJCNN 2019). <a href="http://arxiv.org/abs/1901.06181" target="_blank">[Paper]</a></li>
                <br />

                <li><b>Zapata-Impata, B. S.</b>, Gil, P., & Torres, F. (2019). <i>vision2tactile: Feeling Touch by Sight.</i> In Robotics: Science and Systems (RSS 2019). Workshop on Closing the Reality Gap in Sim2real Transfer for Robotic Manipulation. <a href="https://sim2real.github.io/assets/papers/zapata.pdf" target="_blank">[Paper]</a><a href="https://github.com/yayaneath/yayaneath.github.io/raw/master/docs/posters/rss-vsion2tactile.pdf" target="_blank">[Poster]</a></li>
                <br />

                <li><b>Zapata-Impata, B. S.</b>, Shah, V., Singh, H., & Platt, R. (2019). <i>Approaching Autonomous Open World Transportation.</i> In International Conference on Robotics and Automation (ICRA 2019). Workshop on High Accuracy Mobile Manipulation in Challenging Environments. <a href="https://arxiv.org/abs/1810.03400" target="_blank">[Paper]</a><a href="https://github.com/yayaneath/yayaneath.github.io/raw/master/docs/posters/icra-autotrans.pdf" target="_blank">[Poster]</a></li>
                <br />

                <li><b>Zapata-Impata, B. S.</b>, Gil, P., Garcia-garcia, A., Orts-escolano, S., & Garcia-rodriguez, J. (2019). <i>Tactile Graphs for Grasp Stability Prediction.</i> In International Conference on Learning Representations (ICLR 2019). Workshop on Representation Learning on Graphs and Manifolds. <a href="https://rlgm.github.io/papers/7.pdf" target="_blank">[Paper]</a><a href="https://github.com/yayaneath/yayaneath.github.io/raw/master/docs/posters/iclr-tactilegcn.pdf" target="_blank">[Poster]</a></li>
                <br />

                <li>Castro-Vargas, J. A., <b>Zapata-Impata, B. S.</b>, Gil, P., Garcia-Rodriguez, J., & Torres, F. (2019). <i>3DCNN Performance in Hand Gesture Recognition Applied to Robot Arm Interaction.</i> In 8th International Conference on Pattern Recognition Applications and Methods (ICPRAM 2019). <a href="http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0007570208020806" target="_blank">[Paper]</a></li>
                <br />

                <li><b>Zapata-Impata, B. S.</b>, Gil, P., & Torres, F. (2018). <i>Non-Matrix Tactile Sensors: How Can Be Exploited Their Local Connectivity For Predicting Grasp Stability?</i> In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2018). Workshop on RoboTac: New Progress in Tactile Perception and Learning in Robotics. <a href="http://arxiv.org/abs/1809.05551" target="_blank">[Paper]</a></li>
                <br />

                <li><b>Zapata-Impata, B. S.</b>, Mateo, C. M., Gil, P., & Pomares, J. (2017). <i>Using Geometry to Detect Grasping Points on 3D Unknown Point Cloud.</i> In Proceedings of the 14th International Conference on Informatics in Control, Automation and Robotics (ICINCO 2017) (Vol. 2, pp. 154–161). SCITEPRESS - Science and Technology Publications. Best Paper Award. <a href="https://doi.org/10.5220/0006470701540161" target="_blank">[Paper]</a></li>
            </ul>
        </div>
        <!-- End .optional-section-block -->

    </div>
    <!-- End .optional-section -->

    <footer>
        <div class="container">
            <div class="row">
                <div class="col-sm-5 copyright">
                    <p>
                        Theme by <a href="https://github.com/RyanFitzgerald" target="_blank">Ryan Fitzgerald</a>
                    </p>
                </div>
                <div class="col-sm-2 top">
                    <span id="to-top">
                        <i class="fa fa-chevron-up" aria-hidden="true"></i>
                    </span>
                </div>
            </div>
        </div>
    </footer>
    <!-- End footer -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="js/scripts.min.js"></script>
</body>

</html>
