<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Brayan Impata</title>
	<link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,900" rel="stylesheet">
    <link rel="stylesheet" href="libs/font-awesome/css/font-awesome.min.css">
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/styles.css" rel="stylesheet">
</head>

<body>
    <div id="mobile-menu-open" class="shadow-large">
        <i class="fa fa-bars" aria-hidden="true"></i>
    </div>
    <!-- End #mobile-menu-toggle -->
    <header>
        <div id="mobile-menu-close">
            <span>Close</span> <i class="fa fa-times" aria-hidden="true"></i>
        </div>
        <ul id="menu" class="shadow">
            <li>
                <a href="#about">About</a>
            </li>
            <li>
                <a href="#experience">Experience</a>
            </li>
            <li>
                <a href="#education">Education</a>
            </li>
            <li>
                <a href="#projects">Projects</a>
            </li>
            <li>
                <a href="#skills">Skills</a>
            </li>
            <li>
                <a href="#publications">Publications</a>
            </li>
        </ul>
    </header>
    <!-- End header -->

    <div id="lead">
        <div id="lead-content">
            <img src="images/profile.png" height="290px" width="290px" style="margin-bottom: 15px">
            <h1 style="margin-bottom: 15px">Brayan Impata</h1>
            <h2 style="color:white">Robotics Research Engineer</h2>
            <center>
                <div>
                    <a href="https://twitter.com/_yayan24" target="_blank"><img src="images/twitter.svg" height="65px" width="65px" style="float: center; vertical-align:middle; padding: 6px"></a>
                    <a href="https://github.com/yayaneath" target="_blank"><img src="images/github.svg" height="65px" width="65px" style="float: center; vertical-align:middle; padding: 6px"></a>
                    <a href="https://www.linkedin.com/in/brayan-stiven-zapata-impata" target="_blank"><img src="images/linkedin.svg" height="65px" width="65px" style="float: center; vertical-align:middle; padding: 6px"></a>
                </div>
            </center>
        </div>
        <!-- End #lead-content -->

        <div id="lead-overlay"></div>

        <div id="lead-down">
            <span>
                <i class="fa fa-chevron-down" aria-hidden="true"></i>
            </span>
        </div>
        <!-- End #lead-down -->
    </div>
    <!-- End #lead -->

    <div id="about">
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <h2 class="heading">About Me</h2>
                </div>
                <div class="col-md-8">
                    <p>
                        I am Brayan S. Zapata-Impata, a PhD student at the University of Alicante (Spain) working in <b>Robotic Grasping and Machine Learning</b>. I am mainly interested in providing robots with manipulation skills which are natural for us humans, like finding ways to grasp objects on the fly using <b>visual perception</b> and checking the state of a grasp with <b>tactile perception</b> for keeping a stable grip. We apply this everyday in our lives, even without thinking about it consciously. Delivering robots with such autonomous abilities would have a great impact on a wide set of scenarios: from re-stocking robots to space explorers as well as robots helping us at home.
                        <br />
                        <br />
                        I consider myself an extroverted person who knows that team work is paramount for achieving big goals. I am passionate about learning new things and I am continuously looking for challenges. During my free time <b>I love reading books, listening and playing music, and walking my dog</b>.
                    </p>
                    <br />
                    <a class="btn-rounded-white" href="https://github.com/yayaneath/yayaneath.github.io/raw/master/docs/cv-brayan-impata.pdf" target="_blank">Download Resume</a>
                </div>
            </div>
        </div>
    </div>
    <!-- End #about -->

    <div id="experience" class="background-alt">
        <h2 class="heading">Experience</h2>
        <div id="experience-timeline">
            <div data-date="April 2017 – Present">
                <h3>Robotics Research Engineer</h3>
                <h4>University of Alicante</h4>
                <p>
                    Involved in both research and engineering tasks. I am in charge of designing robotics experiments, developing the entire stack (from hardware to software), evaluating and reporting results. This work is mainly focused on robotic grasping, grasp assessment and control using visual and tactile perception. To achieve this, I am applying learning techniques such as CNNs, LSTMs and GANs in order to provide robots with autonomy and dexterity.
                </p>
            </div>

            <div data-date="May 2018 – September 2018">
                <h3>Visiting Researcher</h3>
                <h4>Khoury College of Computer Sciences</h4>
                <p>
                    Research stay in The Helping Hands Laboratory, head by Dr. Robert Platt, at the Northeastern University (Boston, USA). During my stay, I developed a mobile manipulator system, focusing mainly on providing it with autonomy through the software (task planning, grasping, mobility). I was also in charge of evaluating it on the task of collecting and transporting a variety of objects in a challenging environment outdoors.
                </p>
            </div>

            <div data-date="March 2018 – May 2018">
                <h3>Computer Vision Consultant</h3>
                <h4>Critical Future LTD</h4>
                <p>
                    I led a computer vision project for a healthcare company, being response for the design and implementation of a working solution for classifying skin moles in order to predict skin cancer. This was achieved training with open source databases an ensemble of machine learning models, including CNNs, Gradient Boosting Trees and Support Vector Machines.
                </p>
            </div>

            <div data-date="July 2015 – December 2016">
                <h3>Business Intelligence Developer</h3>
                <h4>Teralco</h4>
                <p>
                    My work involved the maintenance of multidimensional Redshift databases on Amazon Web Services (AWS) as well as the analysis of new features to the models. I was also in charge of Extract/Transform/Load (ETL) and reporting processes. Occasionally, I supported Data Mining and Machine Learning projects collaborating on the analysis and proposing ways to apply AI.
                </p>
            </div>

            <div data-date="September 2014 – April 2015">
                <h3>Undergraduate Intern</h3>
                <h4>University of Alicante</h4>
                <p>
                    I worked on Artificial Intelligence algorithms that could train a more robust Machine Learning model. The idea was to synthetically augment a tabular dataset with more samples using Swarm Intelligence (PSO). We tested it on a dataset for the risk evaluation of congenital hearth surgery on children. After training various models (k-NN, MLP and SVM) with the augmented dataset we found that it was actually possible to improve their performance this way.
                </p>
            </div>

            <div data-date="January 2014 – July 2014">
                <h3>Undergraduate Intern</h3>
                <h4>University of Alicante</h4>
                <p>
                    I worked on the automatic detection of groups of cells that grew on the surface of intra-ocular lens. Our main goal was to measure the lens occupancy degree by the cells. To do so, we processed a picture of the patient's eye using 2D algorithms for segmenting and correctly clustering these cells. 
                </p>
            </div>
        </div>
    </div>
    <!-- End #experience -->

    <div id="education">
        <h2 class="heading">Education</h2>
        <div class="education-block">
            <h3>University of Alicante</h3>
            <span class="education-date">October 2016 - Present</span>
            <h4>PhD in Robotics and Machine Learning</h4>
            <p>
                I mainly research on Robotic Manipulation regarding contact points calculus, grasp planning, grasp assessment, in-hand object state check and dexterous manipulation. In order to accomplish this, I work on Machine Learning and Computer Vision applied to these problems like Object Detection, Image Classification and Pattern Recognition using 2D images and 3D point clouds.
            </p>
        </div>
        <!-- End .education-block -->

        <div class="education-block">
            <h3>University of Alicante</h3>
            <span class="education-date">October 2015 - February 2017</span>
            <h4>Master of Science in Computer Engineering</h4>
            <p>
                The main areas I studied in this master were related to the management of Information Technologies (IT) projects and the innovative application of them. I specialised in the application of IT in R&D, including applied Artificial Intelligence. These studies led me to develop a Master thesis, graded with honours, on the design and implementation of a tool for the re-utilisation of Open Research data in Machine Learning projects (like TensorFlow models).
            </p>
        </div>
        <!-- End .education-block -->

        <div class="education-block">
            <h3>University of Alicante</h3>
            <span class="education-date">September 2011 - July 2015</span>
            <h4>Bachelor of Science in Computer Engineering</h4>
            <p>
                I specialised on research areas like Data Mining, Computer Vision, Robotics and Artificial Intelligence. These studies led me to develop a Bachelor thesis, graded with Honours, on the application of Swarm Intelligence techniques, like Particle Swarm Optimization (PSO), for synthetically augmenting a tabular dataset of clinical records in order to improve the performance of a clinical decision support system.
            </p>
        </div>
        <!-- End .education-block -->
    </div>
    <!-- End #education -->

    <div id="projects" class="background-alt">
        <h2 class="heading">Projects</h2>
        <div class="container">
            <div class="row">
                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/geograsp.jpg" width="300px" height="300px" />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h3>Grasping Points Calculus: GeoGrasp</h3>
                        <p>
                            <b>GeoGrasp</b> is one of the first results of my PhD. It is a method for computing grasping points on partial views of objects using 3D point clouds. It performs a fast geometrical analysis which generalises to a wide range of objects. Consequently, no learning phase is required and it can pe rapidly applied on various scenarios. Right now, it is being used as the base grasping system for the European project <a href="http://commandia.unizar.es/" target="_blank">COMMANDIA</a>. I currently maintain it as an open-source ROS package written in C++.
                        </p>

                        <a href="https://github.com/yayaneath/GeoGrasp" target="_blank">View Project Page</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->

                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/grasp-stability.jpg" width="300px" height="300px" />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h3>Stability Prediction based on Tactile Images and Graphs</h3>
                        <p>
                            Computing visually promising grasps is not enough for developing an autonomous robot. In this project, I have worked on the task of predicting the outcome of a grasp before lifting using the BioTac SP tactile sensors and the Shadow Robot hand. To this end, I have explored two ways for representing the tactile responses: building tactile images or creating graphs that encode the actual spatial distribution of the sensing points.
                        </p>
                        <a href="https://github.com/yayaneath/biotac-sp-images" target="_blank">View Project Page (Images)</a><span style="display:inline-block; width: 15px;"></span><a href="https://github.com/3dperceptionlab/tactile-gcn" target="_blank">View Project Page (Graphs)</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->

                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/task-dos.jpg" width="300px" height="300px" />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h3>Direction of Slip Detection with Temporal Tactile Data</h3>
                        <p>
                            I have explored further the possibilities of tactile perception for grasp assessment. It is important for the autonomy of a robot to recognise whether a grasped object is slipping and also detect in which direction. This would help the system trigger appropriate re-grasping strategies for keeping a stable grip. This problem was approached using temporal sequences of tactile responses for learning to detect up to 7 different slippage patterns.
                        </p>

                        <a href="https://github.com/yayaneath/BioTacSP-DoS" target="_blank">View Project Page</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->

                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/autotrans.jpg" width="300px" height="300px" />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h3>AutOTrans: Autonomous Open world Transportation System</h3>
                        <p>
                            In many cities, trash bags often accumulate through the week, waiting to be picked up. Farm workers must often pick up and carry heavy tools daily. Construction workers spend a lot of time transporting materials through the construction site. These are all labor intensive tasks that could benefit from mobile robotic manipulation. For this project, I focused on the problem of picking and dropping novel objects in an open world environment. This work was carried during a research internship at the <a href="http://www.ccs.neu.edu/home/rplatt/" target="_blank">Helping Hands Lab head by Dr. Robert Platt</a>.
                        </p>

                        <a href="https://github.com/yayaneath/autotrans" target="_blank">View Project Page</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->

                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/touch-sight.jpg" width="300px" height="300px" />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h3>Feeling Touch by Sight</h3>
                        <p>
                            Latest trends in robotic grasping combine vision and touch for improving the performance of systems at tasks like stability prediction. However, tactile data are only available during the grasp, limiting the set of scenarios in which multi-modal solutions can be applied. Could we obtain it prior to grasping? We humans can estimate the feeling of objects without even making contact with them! In this on-going project I work on novel ways for generating robotic tactile data from visual perception.
                        </p>

                        <a href="https://github.com/yayaneath/vision2tactile" target="_blank">View Project Page</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->

                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/rl.jpg" width="300px" height="300px" />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h3>Reinforcement Learning</h3>
                        <p>
                            Humans are expert manipulators. Our dexterity is something we train since we are born: babies continuously use their hands to reach objects and learn to grasp them. Inspired in this fact, researchers are exploring the application of Reinforcement Learning (RL) for teaching robots how to grasp objects. In my path to learn about this field, I am implementing by myself RL techniques and testing them on common robotics benchmarks like a balancing robot. My goal is to develop my understanding of the field and apply these techniques for learning dexterous manipulation skills like re-grasping strategies based on tactile observations.
                        </p>
                        <a href="https://github.com/yayaneath/into-rl" target="_blank">View Project Page</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->
            </div>
        </div>
    </div>
    <!-- End #projects -->

    <div id="skills">
        <h2 class="heading">Skills</h2>
        <ul>
            <li>C</li>
            <li>C++</li>
            <li>Python</li>
            <li>Matlab</li>
            <li>LaTeX</li>
            <li>Linux</li>
            <li>TensorFlow</li>
            <li>Keras</li>
            <li>Pytorch</li>
            <li>ROS</li>
            <li>PCL</li>
            <li>OpenCV</li>
            <li>AWS</li>
            <li>Docker</li>
            <li>Git</li>
            <li>Machine Learning</li>
            <li>Deep Learning</li>
            <li>Reinforcement Learning</li>
            <li>Computer Vision</li>
            <li>Robotics</li>
        </ul>
    </div>
    <!-- End #skills -->

    <div id="publications" class="optional-section background-alt">
        <h2 class="heading">Publications</h2>

        <div class="optional-section-block">
            <h3>Journals</h3>
            <ul>
                <li>Zapata-impata, B. S., Gil, P., & Torres, F. (2019). Tactile-Driven Grasp Stability and Slip Prediction. Robotics, 8(4), 85. <a href="https://doi.org/10.3390/robotics8040085" target="_blank">https://doi.org/10.3390/robotics8040085</a></li>
                <br />
                <li>Zapata-Impata, B. S., Gil, P., Pomares, J., & Torres, F. (2019). Fast geometry-based computation of grasping points on three-dimensional point clouds. International Journal of Advanced Robotic Systems, 16(1). <a href="https://doi.org/10.1177/1729881419831846" target="_blank>">https://doi.org/10.1177/1729881419831846</a></li>
                <br />

                <li>Velasco, E., Zapata-Impata, B. S., Gil, P., & Torres, F. (2019). Object classification using bimodal perception data extracted from single-touch robotic grasps. Revista Iberoamericana de Automatica e Informatica Industrial (RIAI), (April), 1–12. <a href="https://doi.org/10.4995/riai.2019.10923" target="_blank">https://doi.org/10.4995/riai.2019.10923</a>, in press</li>
                <br />

                <li>Zapata-Impata, B. S., Gil, P., & Torres, F. (2019). Learning Spatio Temporal Tactile Features with a ConvLSTM for the Direction Of Slip Detection. Sensors, 19(3), 1–16. <a href="https://doi.org/10.3390/s19030523" target="_blank">https://doi.org/10.3390/s19030523</a></li>
                <br />

                <li>Úbeda, A., Zapata-Impata, B. S., Puente, S. T., Gil, P., Candelas, F., & Torres, F. (2018). A Vision-Driven Collaborative Robotic Grasping System Tele-Operated by Surface Electromyography. Sensors, 18(7), 2366. <a href="https://doi.org/10.3390/s18072366" target="_blank">https://doi.org/10.3390/s18072366</a></li>
            </ul>
        </div>
        <!-- End .optional-section-block -->

        <div class="optional-section-block">
            <h3>Conferences</h3>
            <ul>
                <li>Garcia-Garcia, A., Zapata-Impata, B. S., Orts-Escolano, S., Gil, P., & Garcia-Rodriguez, J. (2019). TactileGCN: A Graph Convolutional Network for Predicting Grasp Stability with Tactile Sensors. In International Joint Conference on Neural Networks (IJCNN 2019). <a href="http://arxiv.org/abs/1901.06181" target="_blank">[Paper]</a></li>
                <br />

                <li>Zapata-Impata, B. S., Gil, P., & Torres, F. (2019). vision2tactile: Feeling Touch by Sight. In Robotics: Science and Systems (RSS 2019). Workshop on Closing the Reality Gap in Sim2real Transfer for Robotic Manipulation. <a href="https://sim2real.github.io/assets/papers/zapata.pdf" target="_blank">[Paper]</a><a href="https://github.com/yayaneath/yayaneath.github.io/raw/master/docs/posters/rss-vsion2tactile.pdf" target="_blank">[Poster]</a></li>
                <br />

                <li>Zapata-Impata, B. S., Shah, V., Singh, H., & Platt, R. (2019). Approaching Autonomous Open World Transportation. In International Conference on Robotics and Automation (ICRA 2019). Workshop on High Accuracy Mobile Manipulation in Challenging Environments. <a href="https://arxiv.org/abs/1810.03400" target="_blank">[Paper]</a><a href="https://github.com/yayaneath/yayaneath.github.io/raw/master/docs/posters/icra-autotrans.pdf" target="_blank">[Poster]</a></li>
                <br />

                <li>Zapata-Impata, B. S., Gil, P., Garcia-garcia, A., Orts-escolano, S., & Garcia-rodriguez, J. (2019). Tactile Graphs for Grasp Stability Prediction. In International Conference on Learning Representations (ICLR 2019). Workshop on Representation Learning on Graphs and Manifolds. <a href="https://rlgm.github.io/papers/7.pdf" target="_blank">[Paper]</a><a href="https://github.com/yayaneath/yayaneath.github.io/raw/master/docs/posters/iclr-tactilegcn.pdf" target="_blank">[Poster]</a></li>
                <br />

                <li>Castro-Vargas, J. A., Zapata-Impata, B. S., Gil, P., Garcia-Rodriguez, J., & Torres, F. (2019). 3DCNN Performance in Hand Gesture Recognition Applied to Robot Arm Interaction. In 8th International Conference on Pattern Recognition Applications and Methods (ICPRAM 2019). <a href="http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0007570208020806" target="_blank">[Paper]</a></li>
                <br />

                <li>Zapata-Impata, B. S., Gil, P., & Torres, F. (2018). Non-Matrix Tactile Sensors: How Can Be Exploited Their Local Connectivity For Predicting Grasp Stability? In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2018). Workshop on RoboTac: New Progress in Tactile Perception and Learning in Robotics. <a href="http://arxiv.org/abs/1809.05551" target="_blank">[Paper]</a></li>
                <br />

                <li>Zapata-Impata, B. S., Mateo, C. M., Gil, P., & Pomares, J. (2017). Using Geometry to Detect Grasping Points on 3D Unknown Point Cloud. In Proceedings of the 14th International Conference on Informatics in Control, Automation and Robotics (ICINCO 2017) (Vol. 2, pp. 154–161). SCITEPRESS - Science and Technology Publications. Best Paper Award. <a href="https://doi.org/10.5220/0006470701540161" target="_blank">[Paper]</a></li>
            </ul>
        </div>
        <!-- End .optional-section-block -->

    </div>
    <!-- End .optional-section -->

    <footer>
        <div class="container">
            <div class="row">
                <div class="col-sm-5 copyright">
                    <p>
                        Theme by <a href="https://github.com/RyanFitzgerald" target="_blank">Ryan Fitzgerald</a>
                    </p>
                </div>
                <div class="col-sm-2 top">
                    <span id="to-top">
                        <i class="fa fa-chevron-up" aria-hidden="true"></i>
                    </span>
                </div>
            </div>
        </div>
    </footer>
    <!-- End footer -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="js/scripts.min.js"></script>
</body>

</html>
